{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üéâ Welcome to Building Your Reference Panel! üéâ\n",
    "\n",
    "---\n",
    "\n",
    "###  **Let‚Äôs get started!** Building a reference panel is an exciting and crucial step in your analysis. Here‚Äôs a roadmap to guide you through the process:\n",
    "\n",
    "---\n",
    "\n",
    "## **Locate Your Open Access Data**  \n",
    "The first step is to identify where your open-access data currently resides.  \n",
    "- **What does this mean?**  \n",
    "  - You‚Äôll need to find the links or sources for the datasets you plan to use.  \n",
    "\n",
    "\n",
    "**Pro tip:**  \n",
    "- Keep track of these links in a well-organized document or spreadsheet. You'll thank youreself later! \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **You‚Äôre off to a great start!**  \n",
    "Finding and organizing your data sources is the foundation of building a robust reference panel. Let‚Äôs dive in!!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Download your data and take note where it lives!\n",
    "wget -nc LINK TO YOUR DATA\n",
    "wget -nc ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# We want to have all our data in the same file format, for the proccesses we are going to do together PLINK's BED format is going to be the best for our purposes. \n",
    "\n",
    "# Understanding PLINK Binary Files (BED/BIM/FAM)\n",
    "\n",
    "You are going to see three files with the same prefix and they are all important in their own way! :\n",
    "\n",
    "## 1. BED (Binary PED file)\n",
    "- A compact--**binary** format for storing genotype data (you will not be able to open this one).\n",
    "- Each SNP is encoded using **two bits per individual** (homozygous major, heterozygous, homozygous minor, or missing).\n",
    "\n",
    "## 2. BIM (Binary Marker Information file)\n",
    "- A **map file** describing **SNPs** in your dataset.\n",
    "- Contains six columns:\n",
    "  1. **Chromosome**\n",
    "  2. **SNP ID**\n",
    "  3. **Genetic distance** (often set to 0)\n",
    "  4. **Physical position** (The location on the genome)\n",
    "  5. **Allele 1** (reference allele)\n",
    "  6. **Allele 2** (alternative allele)\n",
    "\n",
    "## 3. FAM (Family Information file)\n",
    "- A **sample metadata file** describing individuals.\n",
    "- Contains six columns:\n",
    "  1. **Family ID**\n",
    "  2. **Individual ID**\n",
    "  3. **Paternal ID** (0 if unknown)\n",
    "  4. **Maternal ID** (0 if unknown)\n",
    "  5. **Sex** (1 = male, 2 = female, 0 = unknown)\n",
    "  6. **Phenotype** (1 = unaffected, 2 = affected, -9 = missing)\n",
    "\n",
    "## What These Files Tell You\n",
    "- The **BED file** tells you the **genotypes** of individuals.\n",
    "- The **BIM file** tells you **which SNPs** those genotypes correspond to.\n",
    "- The **FAM file** tells you **who the individuals are** and their metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# If you do not have PLINK preinstalled be sure to download it! https://www.cog-genomics.org/plink/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import pysam\n",
    "import subprocess\n",
    "from tabulate import tabulate\n",
    "\n",
    "#If you do not have a package here you can download via the command 'pip install ...' via your command line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Set up plink to work in jupyter notebook (Compute Canada)\n",
    "!module load StdEnv/2020 && module load plink/1.9b_6.21-x86_64 && which plink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the output from above into this next command -- or just the absolute path to your downloaded plink\n",
    "plink_path = 'path/to/plink/command'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the next cell if there is inconsistency in your file formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VCF to Plink Bed\n",
    "\n",
    "subprocess.run([plink_path, \"--vcf\", \"YOUR_FILE\", \"--make-bed\", \"--real-ref-alleles\", \"--out\", \"NAME_YOU_WANT\"], check=True)\n",
    "#Eigensoft file to plink_BED \n",
    "!git clone https://github.com/roberta-davidson/ADMIXTURE-smartPCA-PLINK-and-EIGENSOFT.git\n",
    "!ADMIXTURE-smartPCA-PLINK-and-EIGENSOFT/CONVERTF_EIG_to_PLINK.sh YOUR FILE PREFIX\n",
    "!rm -r ADMIXTURE-smartPCA-PLINK-and-EIGENSOFT\n",
    "\n",
    "#If data is provided as a matrix text file (vintage!) Please refer to matrix_to_vcf.py and matrix_to_vcf.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üöÄ LIFTOVER!  \n",
    "\n",
    "## What is a Genome Build?  \n",
    "A **genome build** is a **reference assembly** of a species' genome, providing a **standardized coordinate system** for genetic variants. Each build is an improved version of previous assemblies, correcting errors, adding missing sequences, and improving accuracy.  \n",
    "\n",
    "### Human Genome Builds:  \n",
    "- **hg18 (NCBI36)** ‚Äì Released in **2006**  \n",
    "- **hg19 (GRCh37)** ‚Äì Released in **2009**  \n",
    "- **hg38 (GRCh38)** ‚Äì Released in **2013** (**Current Build**)  \n",
    "\n",
    "---\n",
    "\n",
    "## Why LiftOver?  \n",
    "You wouldn‚Äôt use an outdated **operating system** on your computer, right?  \n",
    "Similarly, we want to **update** our genomic data to the latest **genome build** to ensure accuracy.  \n",
    "\n",
    "This process is called **\"Lifting Over\"** because we **Liftover** our old data to the newest build.  \n",
    "\n",
    "### Failing to lift over can result in:  \n",
    "‚ùå **Mismatched variant positions**  \n",
    "‚ùå **Incorrect gene annotations**  \n",
    "‚ùå **Data incompatibility** with current research tools  \n",
    "\n",
    "---\n",
    "\n",
    "## Let's Gather Our Tools!  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg19/liftOver/hg19ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/gbdb/hg18/liftOver/hg18ToHg38.over.chain.gz\n",
    "wget -nc https://hgdownload.soe.ucsc.edu/goldenPath/currentGenomes/Homo_sapiens/bigZips/hg38.fa.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are a lot of different tools to perform the liftover function (so many choices!). This tutorial is going to utilize CrossMap (you can find the documentation here: https://crossmap.readthedocs.io/en/latest/). But go ahead and find a software and switch the cod up if you would like; this is your journey! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/liguowang/CrossMap.git\n",
    "#Copy the path where this was downloaded below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossMap_path = '/path/to/CrossMap.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets set up a class in python to peform our initial liftover and then we will reconvene! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shared QC table to track all studies\n",
    "shared_qc_table = []\n",
    "\n",
    "def count_variants(study_name, bim_file, fam_file, step_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        study_name : Name of the study (e.g., \"Study1\").\n",
    "        bim_file : Path to the BIM file.\n",
    "        fam_file : Path to the FAM file.\n",
    "        step_name : Name of the step (e.g., \"Start\", \"After Class1\", \"After Class2\").\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing counts for autosomal, X, Y, MT variants,\n",
    "              total individuals, males, females, and ambiguous individuals.\n",
    "    \"\"\"\n",
    "    autosomal = 0\n",
    "    x_chr = 0\n",
    "    y_chr = 0\n",
    "    mt_chr = 0\n",
    "\n",
    "    \n",
    "    with open(bim_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            chrom = parts[0]\n",
    "            if chrom.startswith(\"chr\"):\n",
    "                chrom_clean = chrom.replace(\"chr\", \"\")\n",
    "            else:\n",
    "                chrom_clean = chrom\n",
    "            if chrom_clean in ['X', '23', '25']:\n",
    "                x_chr += 1\n",
    "            elif chrom_clean in ['Y', '24']:\n",
    "                y_chr += 1\n",
    "            elif chrom_clean in ['MT', 'M', '26']:\n",
    "                mt_chr += 1\n",
    "            elif chrom_clean.isdigit():\n",
    "                if 1 <= int(chrom_clean) <= 22:\n",
    "                    autosomal += 1\n",
    "\n",
    "    \n",
    "    individuals = 0\n",
    "    males = 0\n",
    "    females = 0\n",
    "    ambiguous = 0\n",
    "    \n",
    "    with open(fam_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            sex_code = int(parts[4])\n",
    "            if sex_code == 1:\n",
    "                males += 1\n",
    "            elif sex_code == 2:\n",
    "                females += 1\n",
    "            elif sex_code == 0:\n",
    "                ambiguous += 1\n",
    "            individuals += 1\n",
    "    \n",
    "    shared_qc_table.append([\n",
    "        study_name,\n",
    "        step_name,\n",
    "        autosomal,\n",
    "        x_chr,\n",
    "        y_chr,\n",
    "        mt_chr,\n",
    "        individuals,\n",
    "        males,\n",
    "        females,\n",
    "        ambiguous,\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"autosomal\": autosomal,\n",
    "        \"x_chr\": x_chr,\n",
    "        \"y_chr\": y_chr,\n",
    "        \"mt_chr\": mt_chr,\n",
    "        \"individuals\": individuals,\n",
    "        \"males\": males,\n",
    "        \"females\": females,\n",
    "        \"ambiguous\": ambiguous,\n",
    "    }\n",
    "headers = [\n",
    "    \"Study Name\", \"Step Name\", \"Autosomal\", \"X Chr\", \"Y Chr\", \"MT Chr\",\n",
    "    \"Individuals\", \"Males\", \"Females\", \"Ambiguous\"\n",
    "]\n",
    "\n",
    "def save_qc_table(filename=\"QC_results.txt\"):\n",
    "    \"\"\"\n",
    "    Saves the shared QC table to a text file.\n",
    "\n",
    "    Args:\n",
    "        filename : Name of the output file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        # Write the header\n",
    "        f.write(\"Study\\tStep\\tAutosomal\\tX_Chr\\tY_Chr\\tMT_Chr\\tIndividuals\\tMales\\tFemales\\tAmbiguous\\n\")\n",
    "        \n",
    "        # Write each row of data\n",
    "        for row in shared_qc_table:\n",
    "            f.write(\"\\t\".join(map(str, row)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LiftoverProcessor:\n",
    "    def __init__(self, study_name, base_name, chain_file, out_dir):\n",
    "        \"\"\"\n",
    "        Initialize the LiftoverProcessor class.\n",
    "\n",
    "        Args:\n",
    "            study_name : Name of the study.\n",
    "            base_name : Base name of the input files (without extensions).\n",
    "            chain_file : Path to the chain file for liftover.\n",
    "            out_dir : Directory where all output files will be saved.\n",
    "        \"\"\"\n",
    "        self.study_name = study_name\n",
    "        self.base_name = base_name\n",
    "        self.chain_file = chain_file\n",
    "        self.out_dir = out_dir\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        # Initialize current BIM and FAM files\n",
    "        self.current_base = base_name  # Use the full path provided in base_name\n",
    "        self.current_bim = f\"{self.current_base}.bim\"\n",
    "        self.current_fam = f\"{self.current_base}.fam\"\n",
    "        self.intermediate_files = []\n",
    "        # Initialize QC table as a list of rows\n",
    "        self.qc_table = [[\"Step\", \"Autosomal\", \"X Chr\", \"Y Chr\", \"MT Chr\", \"Individuals\", \"Males\", \"Females\", \"Ambiguous\"]]\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"Start\")\n",
    "\n",
    "    def check_sex(self, ycount_threshold=(0.3, 0.7)):\n",
    "        \"\"\"Check for sex mismatches and remove problematic samples.\"\"\"\n",
    "        sexcheck_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_sexcheck\")\n",
    "        remove_list = []\n",
    "\n",
    "        # Check if there are sex chromosomes (X or Y) in the dataset\n",
    "        has_sex_chromosomes = False\n",
    "        has_polymorphic_x = False\n",
    "\n",
    "        # Read the BIM file to check for sex chromosomes and polymorphic X loci\n",
    "        with open(f\"{self.current_base}.bim\", 'r') as bim_file:\n",
    "            for line in bim_file:\n",
    "                parts = line.strip().split()\n",
    "                chrom = parts[0]\n",
    "                if chrom in ['23', 'X', '25', 'Y']:  # Check for X or Y chromosomes\n",
    "                    has_sex_chromosomes = True\n",
    "                if chrom in ['23', 'X'] and parts[4] != '0':  # Check for polymorphic X loci\n",
    "                    has_polymorphic_x = True\n",
    "\n",
    "        # If no sex chromosomes or no polymorphic X loci, skip sex check\n",
    "        if not has_sex_chromosomes or not has_polymorphic_x:\n",
    "            print(\"No sex chromosomes or no polymorphic X loci detected. Skipping sex check.\")\n",
    "            return\n",
    "\n",
    "        # Run PLINK sex check\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--check-sex', str(ycount_threshold[0]), str(ycount_threshold[1]), '--out', sexcheck_base], check=True)\n",
    "\n",
    "        # Identify problematic samples\n",
    "        with open(f\"{sexcheck_base}.sexcheck\", 'r') as f:\n",
    "            next(f)  # Skip header\n",
    "            for line in f:\n",
    "                if 'PROBLEM' in line:\n",
    "                    parts = line.strip().split()\n",
    "                    remove_list.append(f\"{parts[0]}\\t{parts[1]}\")\n",
    "\n",
    "        # Print problematic samples\n",
    "        print(\"Samples with sex discrepancies:\")\n",
    "        for sample in remove_list:\n",
    "            print(sample)\n",
    "\n",
    "        # Remove problematic samples using PLINK\n",
    "        new_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_good_sex\")\n",
    "        remove_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_remove_badsex.txt\")\n",
    "        with open(remove_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(remove_list))\n",
    "\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--remove', remove_file, '--merge-x', 'no-fail', '--make-bed', '--output-chr', 'MT', '--out', new_base], check=True)\n",
    "\n",
    "        # Clean up intermediate files\n",
    "        self.intermediate_files.extend([remove_file])\n",
    "\n",
    "\n",
    "        # Update current BIM/FAM and count\n",
    "        self.current_base = new_base\n",
    "        self.current_bim = f\"{new_base}.bim\"\n",
    "        self.current_fam = f\"{new_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"Sex Mismatch\")\n",
    "\n",
    "    def create_bed_file(self):\n",
    "        \"\"\"Create a BED file for liftover.\"\"\"\n",
    "        bed_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_bedfile\")\n",
    "        with open(self.current_bim, 'r') as f_in, open(bed_file, 'w') as f_out:\n",
    "            for line in f_in:\n",
    "                parts = line.strip().split()\n",
    "                chrom = parts[0]\n",
    "                pos = int(parts[3])\n",
    "                var_id = parts[1]\n",
    "                chrom_clean = f\"chr{chrom}\" if not chrom.startswith('chr') else chrom\n",
    "                f_out.write(f\"{chrom_clean}\\t{pos-1}\\t{pos}\\t{var_id}\\n\")\n",
    " \n",
    "        self.bedfile_path = bed_file\n",
    "    def run_liftover(self):\n",
    "        # Define output files\n",
    "        map_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_mapfile\")\n",
    "        unmapped_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_unmapped\")\n",
    "        exclude_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_excludefile\")\n",
    "\n",
    "        # Perform the liftover using CrossMap\n",
    "        subprocess.run([CrossMap_path, \"bed\", self.chain_file, self.bedfile_path, map_file, \"--unmap-file\", unmapped_file], check=True)\n",
    "\n",
    "        def extract_variants_to_exclude(input_file, exclude_f):\n",
    "            if os.path.exists(input_file):\n",
    "                with open(input_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if not line.startswith('#'):  # Skip header lines\n",
    "                            parts = line.strip().split()\n",
    "                            if len(parts) >= 4:  # Ensure the line has enough columns\n",
    "                                chrom = parts[0]\n",
    "                                var_id = parts[3]\n",
    "                                # Exclude non-standard chromosomes or unmapped variants\n",
    "                                if input_file == unmapped_file or '_' in chrom or 'chrUn' in chrom or 'chr_alt' in chrom:\n",
    "                                    exclude_f.write(f\"{var_id}\\n\")\n",
    "\n",
    "        # Process the output files to create an exclude list\n",
    "        with open(exclude_file, 'w') as exclude_f:\n",
    "            # Process unmapped_file and map_file\n",
    "            extract_variants_to_exclude(unmapped_file, exclude_f)\n",
    "            extract_variants_to_exclude(map_file, exclude_f)\n",
    "\n",
    "        # Save the final mapped file (only standard chromosomes)\n",
    "        map_file_final = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_mapfile_final\")\n",
    "        with open(map_file_final, 'w') as final_f:\n",
    "            if os.path.exists(map_file):\n",
    "                with open(map_file, 'r') as mapped_f:\n",
    "                    for line in mapped_f:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 4:  # Ensure the line has enough columns\n",
    "                            chrom = parts[0]\n",
    "                            # Include only standard chromosomes\n",
    "                            if '_' not in chrom and 'chrUn' not in chrom and 'chr_alt' not in chrom:\n",
    "                                final_f.write(line)\n",
    "\n",
    "        # Update class attributes\n",
    "        self.mapfile_path = map_file_final\n",
    "        self.exclude_file = exclude_file\n",
    "\n",
    "        # Clean up intermediate files\n",
    "        self.intermediate_files.extend([map_file, unmapped_file])\n",
    "\n",
    "\n",
    "    def update_plink_files(self):\n",
    "        \"\"\"Update PLINK files after liftover.\"\"\"\n",
    "        temp_base = os.path.join(self.out_dir, f\"temp_{os.path.basename(self.base_name)}\")\n",
    "        new_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_38_merged\")\n",
    "\n",
    "        # Exclude problematic variants\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--exclude', self.exclude_file, '--make-bed', '--out', temp_base], check=True)\n",
    "\n",
    "        # Prepare chromosome and position updates\n",
    "        chr_update_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_chr_update.txt\")\n",
    "        pos_update_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_pos_update.txt\")\n",
    "\n",
    "        with open(chr_update_file, 'w') as f_chr, open(pos_update_file, 'w') as f_pos:\n",
    "            for line in open(self.mapfile_path, 'r'):\n",
    "                parts = line.strip().split()\n",
    "                var_id = parts[3]\n",
    "                chrom = parts[0].replace('chr', '')\n",
    "                pos = parts[2]\n",
    "\n",
    "                # Convert chromosome names to PLINK format\n",
    "                if chrom == 'X':\n",
    "                    plink_chrom = '23'\n",
    "                elif chrom == 'Y':\n",
    "                    plink_chrom = '24'\n",
    "                elif chrom in ['MT', 'M']:\n",
    "                    plink_chrom = '26'\n",
    "                else:\n",
    "                    plink_chrom = chrom\n",
    "\n",
    "                f_chr.write(f\"{var_id}\\t{plink_chrom}\\n\")\n",
    "                f_pos.write(f\"{var_id}\\t{pos}\\n\")\n",
    "\n",
    "        # Apply updates\n",
    "        subprocess.run([plink_path, '--bfile', temp_base, '--update-chr', chr_update_file, '--update-map', pos_update_file, '--make-bed', '--output-chr', 'chrMT', '--out', new_base], check=True)\n",
    "\n",
    "        # Clean up intermediate files\n",
    "        self.intermediate_files.extend([\n",
    "        chr_update_file,\n",
    "        pos_update_file,\n",
    "        (f\"{temp_base}.bed\"),\n",
    "        (f\"{temp_base}.bim\"),\n",
    "        (f\"{temp_base}.fam\"),\n",
    "        self.mapfile_path])\n",
    "\n",
    "        # Update current files and count\n",
    "        self.current_base = new_base\n",
    "        self.current_bim = f\"{new_base}.bim\"\n",
    "        self.current_fam = f\"{new_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"After liftover\")\n",
    "    \n",
    "        for file in self.intermediate_files:\n",
    "            if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        # This will run the full liftover operation.\n",
    "        self.check_sex()\n",
    "        self.create_bed_file()\n",
    "        self.run_liftover()\n",
    "        self.update_plink_files()\n",
    " \n",
    "\n",
    "    def get_output_files(self):\n",
    "        \"\"\"Return the final BIM and FAM files after liftover.\"\"\"\n",
    "        return self.current_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can do the initial liftover of our data! Exciting stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add as many studies as needed! Just be sure to add them downstream!\n",
    "\n",
    "study1= LiftoverProcessor('Study1','path/data_base_name', 'hg19ToHg38.over.chain.gz', 'output_directory')\n",
    "study2= LiftoverProcessor('Study2','path/data_base_name', 'hg18ToHg38.over.chain.gz', 'output_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1.check_sex()\n",
    "study2.check_sex()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we see any noteable reduction in sample size? If so make sure there is not a more sytematic problem with your data that may need some manual fixing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1.create_bed_file()\n",
    "study2.create_bed_file()\n",
    "study1.run_liftover()\n",
    "study2.run_liftover()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have lift off! Now lets really get this car into gear and update our plink files to reflect our new mapped variants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1.update_plink_files()\n",
    "study2.update_plink_files()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "# Ok! We have successfully performed a liftover! How do you feel? While the literal process of lifting over the coordinates has been completed, we are not quite done:\n",
    "    \n",
    "There‚Äôs still some housekeeping to take care of‚Äîafter all, we didn‚Äôt generate this data ourselves, so we need to make sure everything‚Äôs in order. Here‚Äôs the game plan: \n",
    "\n",
    "---\n",
    "- **Map to Reference FASTA**: \n",
    "    - Let‚Äôs make sure those lifted-over variants actually match the sequence of the new genome build.\n",
    "- **Strand Flips**: \n",
    "    - Keep an eye out for major/minor (reference/alternate) allele mix-ups. Sometimes the reference strand orientation flips between builds, and we need to straighten that out.\n",
    "- **Palindromic SNPs**: \n",
    "    - These variants are indistinguishable on forward and reverse strands, making them a headache for allele assignment. Since they‚Äôre too ambiguous to resolve confidently, we‚Äôll filter them out entirely\n",
    "- **Invalid SNPs**: \n",
    "    - Last but not least, filter out any variants where the reference allele doesn‚Äôt match the new build. Only the real deal makes the cut!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReferenceAligner:\n",
    "    def __init__(self, study_name, base_name, genome_fasta, output_directory):\n",
    "        \"\"\"\n",
    "        Initialize the ReferenceAligner class.\n",
    "\n",
    "        Args:\n",
    "            study_name : Name of the study.\n",
    "            base_name : Base name of the input files (without extensions).\n",
    "            genome_fasta : Path to the reference genome FASTA file.\n",
    "            output_directory : Directory where temporary and output files will be saved.\n",
    "        \"\"\"\n",
    "        self.study_name = study_name\n",
    "        self.base_name = base_name\n",
    "        self.genome_fasta = genome_fasta\n",
    "        self.output_directory = output_directory\n",
    "        self.intermediate_files = []\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(self.output_directory, exist_ok=True)\n",
    "\n",
    "        # Initialize current BIM and FAM files\n",
    "        self.current_bim = f\"{self.base_name}.bim\"\n",
    "        self.current_fam = f\"{self.base_name}.fam\"\n",
    "\n",
    "    def strand_flip(self, a):\n",
    "        \"\"\"Helper function to flip alleles.\"\"\"\n",
    "        return {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}[a]\n",
    "\n",
    "    def generate_alignment_files(self):\n",
    "      \n",
    "        #Generate the necessary files for alignment (remove.txt, strand_flip.txt, force_a1.txt).\n",
    "        \n",
    "        n_total_variants = 0\n",
    "        n_non_snps = 0\n",
    "        n_palindromic = 0\n",
    "        n_flip_strand = 0\n",
    "        n_force_ref_allele = 0\n",
    "        n_no_ref_match = 0\n",
    "\n",
    "        # Define paths for output files \n",
    "        self.remove_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.remove.txt\")\n",
    "        self.flip_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.strand_flip.txt\")\n",
    "        self.force_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.force_a1.txt\")\n",
    "\n",
    "        with open(self.current_bim, 'rt') as ibim, pysam.FastaFile(self.genome_fasta) as ifasta, \\\n",
    "            open(self.remove_file, 'wt') as oremove, \\\n",
    "            open(self.flip_file, 'wt') as oflip, \\\n",
    "            open(self.force_file, 'wt') as oforce:\n",
    "\n",
    "            fasta_chroms = set(list(ifasta.references))\n",
    "            for line in ibim:\n",
    "                fields = line.rstrip().split()\n",
    "                chrom, varid, pos, a1, a2 = fields[0], fields[1], int(fields[3]), fields[4], fields[5]\n",
    "            \n",
    "                n_total_variants += 1\n",
    "\n",
    "                # Handle chromosome naming quirks\n",
    "                if chrom not in fasta_chroms:\n",
    "                    chrom = chrom[3:] if chrom.startswith('chr') else f'chr{chrom}'\n",
    "                    if chrom not in fasta_chroms:\n",
    "                        print(f'Warning: skipping chromosome {fields[0]} because it is not in FASTA file.')\n",
    "                        continue\n",
    "\n",
    "                # Skip non-SNPs\n",
    "                if a1 not in {'A', 'C', 'G', 'T'} or a2 not in {'A', 'C', 'G', 'T'}:\n",
    "                    oremove.write(f'{varid}\\n')\n",
    "                    n_non_snps += 1\n",
    "                    continue\n",
    "\n",
    "                # Skip palindromic SNPs\n",
    "                if (a1 in {'A', 'T'} and a2 in {'A', 'T'}) or (a1 in {'C', 'G'} and a2 in {'C', 'G'}):\n",
    "                    oremove.write(f'{varid}\\n')\n",
    "                    n_palindromic += 1\n",
    "                    continue\n",
    "\n",
    "                # Retreive the reference allele from the hg38 FASTA\n",
    "                ref_base = None\n",
    "                for base in ifasta.fetch(chrom, pos - 1, pos):\n",
    "                    ref_base = base\n",
    "\n",
    "                if ref_base == a2:\n",
    "                    n_force_ref_allele += 1\n",
    "                    oforce.write(f'{varid}\\t{ref_base}\\n')\n",
    "                elif ref_base != a1:\n",
    "                    flipped_a1 = self.strand_flip(a1)\n",
    "                    flipped_a2 = self.strand_flip(a2)\n",
    "                    if ref_base == flipped_a2:\n",
    "                        n_force_ref_allele += 1\n",
    "                        oforce.write(f\"{varid}\\t{ref_base}\\n\")\n",
    "                        n_flip_strand += 1\n",
    "                        oflip.write(f\"{varid}\\n\")\n",
    "                    elif ref_base == flipped_a1:\n",
    "                        n_flip_strand += 1\n",
    "                        oflip.write(f\"{varid}\\n\")\n",
    "                    else:\n",
    "                        n_no_ref_match += 1\n",
    "                        oremove.write(f\"{varid}\\n\")\n",
    "\n",
    "        print(f\"Total variants: {n_total_variants:,}\")\n",
    "        print(f\"Not valid SNPs: {n_non_snps:,}\")\n",
    "        print(f\"Palindromic SNPs: {n_palindromic:,}\")\n",
    "        print(f\"Strand flips: {n_flip_strand:,}\")\n",
    "        print(f\"Force reference allele: {n_force_ref_allele:,}\")\n",
    "        print(f\"A1/A2 didn't match reference allele: {n_no_ref_match:,}\")\n",
    "\n",
    "    def align_to_reference(self):\n",
    "        \"\"\"\n",
    "        Aligns the data to the reference genome and sets HH to missing.\n",
    "        \"\"\"\n",
    "        self.remove_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.remove.txt\")\n",
    "        self.flip_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.strand_flip.txt\")\n",
    "        self.force_file = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}.force_a1.txt\")\n",
    "        # Define paths for intermediate files in the output directory\n",
    "        temp_base = os.path.join(self.output_directory, \"temp\")\n",
    "        temp2_base = os.path.join(self.output_directory, \"temp2\")\n",
    "        aligned_base = os.path.join(self.output_directory, f\"{os.path.basename(self.base_name)}_aligned\")\n",
    "    \n",
    "        # Remove impossible to adjust SNPs\n",
    "        subprocess.run([plink_path, '--bfile', self.base_name, '--exclude', self.remove_file, '--make-bed', '--out', temp_base], check=True)\n",
    "        count_variants(self.study_name, f\"{temp_base}.bim\", self.current_fam, \"After Removal of Impossible SNPs\")\n",
    "\n",
    "        # Strand flip\n",
    "        subprocess.run([plink_path, '--bfile', temp_base, '--flip', self.flip_file, '--make-bed', '--out', temp2_base], check=True)\n",
    "        count_variants(self.study_name, f\"{temp2_base}.bim\", self.current_fam, \"After Strand Flip\")\n",
    "\n",
    "        # Force the REF/ALT designation\n",
    "        subprocess.run([plink_path, '--bfile', temp2_base, '--a1-allele', self.force_file, '--make-bed', '--out', aligned_base], check=True)\n",
    "        count_variants(self.study_name, f\"{aligned_base}.bim\", self.current_fam, \"After Alignment\")\n",
    "    \n",
    "        # Clean up intermediate files\n",
    "        for temp_file in [temp_base, temp2_base]:\n",
    "            for ext in [\".bed\", \".bim\", \".fam\", \".log\", \".nosex\"]:\n",
    "                file_path = f\"{temp_file}{ext}\"\n",
    "                if os.path.exists(file_path):\n",
    "                    self.intermediate_files.append(file_path)\n",
    "\n",
    "        # Add alignment files to intermediate_files list\n",
    "        for file in [self.remove_file, self.flip_file, self.force_file]:\n",
    "            if os.path.exists(file):\n",
    "                self.intermediate_files.append(file)\n",
    "\n",
    "        # Clean up all intermediate files\n",
    "        for file in self.intermediate_files:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "        \n",
    "        self.current_base = aligned_base\n",
    "        self.current_bim = f\"{aligned_base}.bim\"\n",
    "        self.current_fam = f\"{aligned_base}.fam\"\n",
    "        print(f\"Alignment completed for {self.study_name}.\")\n",
    "        \n",
    "    def get_output_files(self):\n",
    "        \"\"\"Return the final BIM and FAM files after liftover.\"\"\"\n",
    "        return self.current_base\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets get to aligning! Be sure to remember where you sent the output of that lift over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take note of your last output from the last section!\n",
    "final_base = study1.get_output_files()\n",
    "print(f\"Final base file: {final_base}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take note of your last output from the last section!\n",
    "final_base = study2.get_output_files()\n",
    "print(f\"Final base file: {final_base}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can feed that to our alignment!\n",
    "\n",
    "---\n",
    "\n",
    "Be sure that your studies are being sent to **different directories** to ensure there is not issues with the alignment process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1_A = ReferenceAligner('Study1', 'path/to/final_bim_base_name', 'path/to/hg38.fa.gz', 'output_directory')\n",
    "study2_A = ReferenceAligner('Study2', 'path/to/final_bim_base_name', 'path/to/hg38.fa.gz', 'output_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1_A.generate_alignment_files()\n",
    "study2_A.generate_alignment_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there was definetely some tidying to do, no matter how clean the data you inherited was!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study1_A.align_to_reference()\n",
    "study2_A.align_to_reference()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We're Almost There! \n",
    "\n",
    "---\n",
    "\n",
    "###  **Great progress so far!** We've tackled the bulk of the challenging work, and now we‚Äôre down to the final steps to ensure our data is polished and ready for standard quality control. Let‚Äôs break it down:\n",
    "\n",
    "---\n",
    "\n",
    "##  **Handling Duplicate Variants**  \n",
    "Occasionally, multiple variants can appear at the same genomic position, which can complicate downstream analysis.  \n",
    "**Our goal:**  \n",
    "- Identify these duplicates and retain only the variant that is most prevalent in our dataset.   \n",
    "\n",
    "---\n",
    "\n",
    "##  **Standardizing Variant IDs**  \n",
    "Different studies often use different genotyping chips, each with its own naming conventions for variant IDs. This inconsistency can create headaches when integrating multiple datasets.  \n",
    "\n",
    "**Our solution:**  \n",
    "- Rename variant IDs using a universal format: **`chr:pos:ALT:REF`**.  \n",
    "- This ensures consistency across studies and makes our data more interoperable. Think of it as giving everyone the same map to follow! üó∫Ô∏è  \n",
    "\n",
    "---\n",
    "\n",
    "##  **Addressing Heterozygous Haploid Variants**  \n",
    "- **What‚Äôs the issue?**  \n",
    "  - PLINK sometimes misinterprets haploid chromosomes (e.g., the male X chromosome) as diploid.  \n",
    "- **Why does this matter?**  \n",
    "  - This can skew analysis or indicate potential data quality issues.  \n",
    "\n",
    "**Our approach:**  \n",
    "- Set these heterozygous haploid variants to **missing** for now.  \n",
    "- We‚Äôll revisit these later to ensure accuracy and robustness in our analysis. Safety first! üõ°Ô∏è  \n",
    "\n",
    "---\n",
    "\n",
    "###  **You‚Äôre doing amazing!**  \n",
    "- These final steps will ensure our data is clean, consistent, and ready for the next phase of analysis. \n",
    "- Let‚Äôs power through and get this done‚Äîyou‚Äôve got this! üí™  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dup_Renamer:\n",
    "    def __init__(self, study_name, base_name, out_dir):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the DeduplicationProcessor class.\n",
    "\n",
    "        Args:\n",
    "            study_name : Name of the study.\n",
    "            base_name : Base name of the input files (including full path, e.g., \"path/to/data_base_name\").\n",
    "            out_dir : Directory where all output files will be saved.\n",
    "        \"\"\"\n",
    "        self.study_name = study_name\n",
    "        self.base_name = base_name  # Full path to the base name (e.g., \"path/to/data_base_name\")\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "\n",
    "        # Initialize current BIM and FAM files\n",
    "        self.current_base = os.path.join(self.out_dir, os.path.basename(base_name))\n",
    "        self.current_bim = f\"{self.current_base}.bim\"\n",
    "        self.current_fam = f\"{self.current_base}.fam\"\n",
    "\n",
    "        # Track intermediate files for cleanup\n",
    "        self.intermediate_files = []\n",
    "\n",
    "    def list_duplicate_vars(self):\n",
    "        \"\"\"Run PLINK to list duplicate variants.\"\"\"\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--list-duplicate-vars', '--out', self.current_base], check=True)\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--freq', 'counts', '--out', self.current_base], check=True)\n",
    "\n",
    "        # Track intermediate files\n",
    "        self.intermediate_files.extend([\n",
    "            f\"{self.current_base}.dupvar\",\n",
    "            f\"{self.current_base}.frq.counts\",\n",
    "            f\"{self.current_base}.log\"\n",
    "        ])\n",
    "\n",
    "    def prioritize_duplicates(self):\n",
    "        \"\"\"Prioritize duplicates based on missingness and generate an exclude list.\"\"\"\n",
    "        dupvar_file = f\"{self.current_base}.dupvar\"\n",
    "        counts_file = f\"{self.current_base}.frq.counts\"\n",
    "        exclude_file = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_exclude_dup.txt\")\n",
    "\n",
    "        missingness = dict()\n",
    "\n",
    "        # Read missingness data from .frq.counts file\n",
    "        with open(counts_file, 'rt') as icounts:\n",
    "            header = icounts.readline().split()\n",
    "            for line in icounts:\n",
    "                fields = dict(zip(header, line.split()))\n",
    "                missingness[fields['SNP']] = int(fields['G0'])\n",
    "\n",
    "        # Prioritize duplicates and write to exclude file\n",
    "        with open(dupvar_file, 'rt') as idupvar, open(exclude_file, 'wt') as ofile:\n",
    "            header = idupvar.readline().split()\n",
    "            for line in idupvar:\n",
    "                var_ids = line.strip().split('\\t')[-1].split()\n",
    "                var_ids_sorted = sorted([(var_id, missingness[var_id]) for var_id in var_ids], key=lambda x: x[1], reverse=True)\n",
    "                for var_id in var_ids_sorted[:-1]:\n",
    "                    ofile.write(var_id[0] + '\\n')\n",
    "\n",
    "        self.exclude_file = exclude_file\n",
    "        self.intermediate_files.append(exclude_file)\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicates using the exclude list.\"\"\"\n",
    "        dedup_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_dedup\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--exclude', self.exclude_file, '--output-chr', 'chrMT', '--keep-allele-order', '--make-bed', '--out', dedup_base], check=True)\n",
    "        self.current_base = dedup_base\n",
    "        self.current_bim = f\"{dedup_base}.bim\"\n",
    "        self.current_fam = f\"{dedup_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"After removal of duplicates\")\n",
    "\n",
    "        # Track intermediate files\n",
    "        self.intermediate_files.extend([\n",
    "            f\"{dedup_base}.bed\",\n",
    "            f\"{dedup_base}.bim\",\n",
    "            f\"{dedup_base}.fam\",\n",
    "            f\"{dedup_base}.log\"\n",
    "        ])\n",
    "\n",
    "    def rename_variants(self):\n",
    "        \"\"\"Rename variants using chr:pos:ALT:REF format.\"\"\"\n",
    "        rename_file = os.path.join(self.out_dir, \"rename.txt\")\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            old_stdout = sys.stdout\n",
    "            sys.stdout = devnull\n",
    "            try:\n",
    "                with open(self.current_bim, 'r') as f_in, open(rename_file, 'w') as f_out:\n",
    "                    for line in f_in:\n",
    "                        parts = line.strip().split()\n",
    "                        chrom, pos, alt, ref = parts[0], parts[3], parts[5], parts[4]\n",
    "                        new_id = f\"chr{chrom}:{pos}:{alt}:{ref}\"\n",
    "                        f_out.write(f\"{new_id} {parts[1]}\\n\")  \n",
    "            finally:\n",
    "                sys.stdout = old_stdout\n",
    "        renamed_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_renamed\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--update-name', rename_file, '1', '2', '--make-bed', '--keep-allele-order', '--output-chr', 'chrMT', '--out', renamed_base], check=True)\n",
    "        self.current_base = renamed_base\n",
    "        self.current_bim = f\"{renamed_base}.bim\"\n",
    "        self.current_fam = f\"{renamed_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"After renaming\")\n",
    "\n",
    "        # Track intermediate files\n",
    "        self.intermediate_files.extend([\n",
    "            rename_file,\n",
    "            f\"{renamed_base}.bed\",\n",
    "            f\"{renamed_base}.bim\",\n",
    "            f\"{renamed_base}.fam\",\n",
    "            f\"{renamed_base}.log\"\n",
    "        ])\n",
    "    def split_and_merge_x(self):\n",
    "        \"\"\"Split and merge X chromosomes.\"\"\"\n",
    "        xsplit_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_Xsplit\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--split-x', 'no-fail', 'b38', '--keep-allele-order', '--make-bed', '--output-chr', 'chrMT', '--out', xsplit_base], check=True)\n",
    "        self.current_base = xsplit_base\n",
    "        self.current_bim = f\"{xsplit_base}.bim\"\n",
    "        self.current_fam = f\"{xsplit_base}.fam\"\n",
    "\n",
    "        xsplit_temp_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_Xsplit_temp\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--keep-allele-order', '--set-hh-missing', '--make-bed', '--output-chr', 'chrMT', '--out', xsplit_temp_base], check=True)\n",
    "        self.current_base = xsplit_temp_base\n",
    "        self.current_bim = f\"{xsplit_temp_base}.bim\"\n",
    "        self.current_fam = f\"{xsplit_temp_base}.fam\"\n",
    "\n",
    "        final_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_hg38\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--merge-x', 'no-fail', '--keep-allele-order', '--make-bed', '--output-chr', 'chrMT', '--out', final_base], check=True)\n",
    "        self.current_base = final_base\n",
    "        self.current_bim = f\"{final_base}.bim\"\n",
    "        self.current_fam = f\"{final_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"Final counts Lift over!\")\n",
    "\n",
    "        # Track intermediate files\n",
    "        self.intermediate_files.extend([\n",
    "            f\"{xsplit_base}.bed\",\n",
    "            f\"{xsplit_base}.bim\",\n",
    "            f\"{xsplit_base}.fam\",\n",
    "            f\"{xsplit_base}.log\",\n",
    "            f\"{xsplit_temp_base}.bed\",\n",
    "            f\"{xsplit_temp_base}.bim\",\n",
    "            f\"{xsplit_temp_base}.fam\",\n",
    "            f\"{xsplit_temp_base}.log\"\n",
    "            \n",
    "        ])\n",
    "\n",
    "        for file in self.intermediate_files:\n",
    "            if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "        print(\"Cleaned up intermediate files.\")\n",
    "        print(f\"LIiftover is all done for {self.study_name}, happy hunting!!\")\n",
    "        \n",
    "\n",
    "\n",
    "    def get_output_files(self):\n",
    "        \"\"\"Return the final BIM and FAM files after deduplication and renaming.\"\"\"\n",
    "        return self.current_base\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take note of your last output from the last section!\n",
    "final_base = study2_A.get_output_files()\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take note of your last output from the last section!\n",
    "final_base = study1_A.get_output_files()\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_D = Dup_Renamer('Study1', '/home/belleza/scratch/1KGHGDP/test_aligner/HGDP1KG_OPT_aligned', '/home/belleza/scratch/1KGHGDP/test_aligner/')\n",
    "Study1_D.list_duplicate_vars()\n",
    "Study1_D.prioritize_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the output directories are different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_D.rename_variants()\n",
    "Study1_D.split_and_merge_x()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Study1_D = Dup_Renamer('Study1', 'path/to/final_bim_base_name', 'output_directory')\n",
    "Study2_D = Dup_Renamer('Study1', 'path/to/final_bim_base_name', 'output_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_D.list_duplicate_vars()\n",
    "Study1_D.prioritize_duplicates()\n",
    "Study2_D.list_duplicate_vars()\n",
    "Study2_D.prioritize_duplicates()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This ***should*** be the last step with any loss of variants, keep an eye out for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_D.rename_variants()\n",
    "Study2_D.rename_variants()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Study1_D.split_and_merge_x()\n",
    "Study2_D.split_and_merge_x()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last step\n",
    "save_qc_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ **You Did It! Look at You Go, Hot Shot!** üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### **Great job!** Our reference data now has that fresh, polished **\"new genomic build smell\"**, and it wasn‚Äôt even that hard, was it? üòé  \n",
    "\n",
    "---\n",
    "\n",
    "## **What‚Äôs Next?**  \n",
    "Now that our data is looking sharp, it‚Äôs time to ensure it‚Äôs in tip-top shape before we bring everything together.  \n",
    "\n",
    "**Here‚Äôs the plan:**  \n",
    "- Perform **standard quality control (QC) measures** on each dataset independently.  \n",
    "- This step is crucial to catch any potential issues early and ensure our data is reliable and ready for integration.  \n",
    "\n",
    "---\n",
    "\n",
    "### **You‚Äôre crushing it!**  \n",
    "Head over to the next section to dive into the QC steps. See you there‚Äîkeep up the fantastic work! üí™  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
