{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "# Wow! We made it to our final notebook together - it went by so fast, huh?\n",
    "\n",
    "# So our data is all cleaned up individually, but now its time to bring them together! \n",
    "---\n",
    "# This tutorial will walk us through the combination of multiple plink BED datasets \n",
    "  - If a singular study has multiple arrays/sets of data - run this tutorial on those intra-study datasets before merging everything together!\n",
    "# This tutorial will process neccesary post-merger quality control measures\n",
    "---\n",
    "# The basic steps of our merger will include:\n",
    "\n",
    "  - **Performing the raw merger**\n",
    "  - **Eliminate non-overlapping variants**\n",
    "  - **Inspect for duplicate samples**\n",
    "  - **Conduct fine-grained batch effect analyses**\n",
    "---\n",
    "# You've got this! Lets get our neccesary packages loaded up and we will get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from qqman import qqman\n",
    "import subprocess\n",
    "import IPython\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "import tempfile\n",
    "import glob\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you do not already have the software Primus downloded (we will be using it later) you can download it from https://primus.gs.washington.edu/primusweb/res/form.html \n",
    "!wget -nc -P primus https://primus.gs.washington.edu/docroot/versions/PRIMUS_v1.9.0.tgz\n",
    "!tar -xvf primus/PRIMUS_v1.9.0.tgz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up plink to work in jupyter notebook (Compute Canada)\n",
    "!module load StdEnv/2020 && module load plink/1.9b_6.21-x86_64 && which plink\n",
    "!module load StdEnv/2020 && module load plink/2.00a3.6 && which plink2\n",
    "!module load StdEnv/2020 && module load gcc/9.3.0 && module load flashpca/2.0 && which flashpca\n",
    "#!wget -P \"Directory\" -nc https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/gap.txt.gz\n",
    "#!gunzip \"Directory\"/gap.txt.gz \n",
    "#!cut -f 2-4,8 \"Directory\"/gap.txt > \"Directory\"/genome_gap_hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you do not already have the software Flashpca you can download it from  https://github.com/gabraham/flashpca.git\n",
    "!git clone https://github.com/gabraham/flashpca.git\n",
    "!cd gabraham\n",
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the output from above into this next command -- or just the absolute path to your downloaded plink\n",
    "plink_path = 'Absolute Path to Plink'\n",
    "plink2_path = 'Absolute Path to Plink 2'\n",
    "primus_path = 'Absolute path to run_PRIMUS.pl'\n",
    "flash_path = 'Absolute path to flashpca' \n",
    "genome_gap = \"<Directory>/genome_gap_hg38.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shared QC table to track all studies\n",
    "shared_qc_table = []\n",
    "\n",
    "def count_variants(study_name, bim_file, fam_file, step_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        study_name : Name of the study (e.g., \"Study1\").\n",
    "        bim_file : Path to the BIM file.\n",
    "        fam_file : Path to the FAM file.\n",
    "        step_name : Name of the step (e.g., \"Start\", \"After Class1\", \"After Class2\").\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing counts for autosomal, X, Y, MT variants,\n",
    "              total individuals, males, females, and ambiguous individuals.\n",
    "    \"\"\"\n",
    "    autosomal = 0\n",
    "    x_chr = 0\n",
    "    y_chr = 0\n",
    "    mt_chr = 0\n",
    "\n",
    "    \n",
    "    with open(bim_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            chrom = parts[0]\n",
    "            if chrom.startswith(\"chr\"):\n",
    "                chrom_clean = chrom.replace(\"chr\", \"\")\n",
    "            else:\n",
    "                chrom_clean = chrom\n",
    "            if chrom_clean in ['X', '23', '25']:\n",
    "                x_chr += 1\n",
    "            elif chrom_clean in ['Y', '24']:\n",
    "                y_chr += 1\n",
    "            elif chrom_clean in ['MT', 'M', '26']:\n",
    "                mt_chr += 1\n",
    "            elif chrom_clean.isdigit():\n",
    "                if 1 <= int(chrom_clean) <= 22:\n",
    "                    autosomal += 1\n",
    "\n",
    "    \n",
    "    individuals = 0\n",
    "    males = 0\n",
    "    females = 0\n",
    "    ambiguous = 0\n",
    "    \n",
    "    with open(fam_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            sex_code = int(parts[4])\n",
    "            if sex_code == 1:\n",
    "                males += 1\n",
    "            elif sex_code == 2:\n",
    "                females += 1\n",
    "            elif sex_code == 0:\n",
    "                ambiguous += 1\n",
    "            individuals += 1\n",
    "    \n",
    "    shared_qc_table.append([\n",
    "        study_name,\n",
    "        step_name,\n",
    "        autosomal,\n",
    "        x_chr,\n",
    "        y_chr,\n",
    "        mt_chr,\n",
    "        individuals,\n",
    "        males,\n",
    "        females,\n",
    "        ambiguous,\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"autosomal\": autosomal,\n",
    "        \"x_chr\": x_chr,\n",
    "        \"y_chr\": y_chr,\n",
    "        \"mt_chr\": mt_chr,\n",
    "        \"individuals\": individuals,\n",
    "        \"males\": males,\n",
    "        \"females\": females,\n",
    "        \"ambiguous\": ambiguous,\n",
    "    }\n",
    "headers = [\n",
    "    \"Study Name\", \"Step Name\", \"Autosomal\", \"X Chr\", \"Y Chr\", \"MT Chr\",\n",
    "    \"Individuals\", \"Males\", \"Females\", \"Ambiguous\"\n",
    "]\n",
    "\n",
    "def save_qc_table(filename=\"Merger_results.txt\"):\n",
    "    \"\"\"\n",
    "    Saves the shared QC table to a text file.\n",
    "\n",
    "    Args:\n",
    "        filename : Name of the output file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        # Write the header\n",
    "        f.write(\"Study\\tStep\\tAutosomal\\tX_Chr\\tY_Chr\\tMT_Chr\\tIndividuals\\tMales\\tFemales\\tAmbiguous\\n\")\n",
    "        \n",
    "        # Write each row of data\n",
    "        for row in shared_qc_table:\n",
    "            f.write(\"\\t\".join(map(str, row)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we dive in, we need to look at the overlap that our data has with each other.\n",
    "# Why is this a concern you may ask?\n",
    "---\n",
    "# In the world of bioinformatics - number of variants is the name of the game!\n",
    "- The more variants we have, the finer and more detailed our analyses can be.\n",
    "# But why exactly do we need to be concerned with overlapping data?\n",
    "---\n",
    "# Different biotechnology companies use different chips to genotype their genetic data\n",
    "- Different chips will focus on different positions and thus we are presented with our problem of overlap.\n",
    "---\n",
    "# While we love and cherish our data, we need to be pragmatic about the overlap of variants while also balancing the demographic needs of our merged dataset!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Move onto the next cell so we can see what we are dealing with in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combo(list_of_bim_files):\n",
    "    \"\"\"\n",
    "    Analyzes the intersection of SNP IDs from multiple .bim files and displays the results in a table.\n",
    "    \"\"\"\n",
    "    \n",
    "    def read_bim_file(bim_file_path):\n",
    "        \"\"\"\n",
    "        Reads a .bim file and returns a set of SNP IDs from the second column.\n",
    "        \"\"\"\n",
    "        snp_ids = set()\n",
    "        try:\n",
    "            with open(bim_file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) > 1:\n",
    "                        snp_ids.add(parts[1])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File {bim_file_path} not found!\")\n",
    "        return snp_ids\n",
    "    \n",
    "    def get_filename(file_path):\n",
    "        \"\"\"\n",
    "        Extracts the file name from the full file path.\n",
    "        \"\"\"\n",
    "        return os.path.basename(file_path)\n",
    "    \n",
    "    # Read SNP IDs from all .bim files\n",
    "    snp_dict = {bim_file: read_bim_file(bim_file) for bim_file in list_of_bim_files}\n",
    "    \n",
    "    # Remove any files that couldn't be read\n",
    "    snp_dict = {k: v for k, v in snp_dict.items() if v}\n",
    "    \n",
    "    if not snp_dict:\n",
    "        print(\"No valid .bim files were successfully read. Exiting.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were read\n",
    "    \n",
    "    file_names = list(snp_dict.keys())\n",
    "    intersections = []\n",
    "    \n",
    "    # Consider all combinations of the files\n",
    "    for r in range(2, len(file_names) + 1):  # Starting from 2 files for intersection\n",
    "        for combo in itertools.combinations(file_names, r):\n",
    "            # Calculate the intersection of the SNP IDs of the files in the combination\n",
    "            combined_snps = snp_dict[combo[0]]\n",
    "            for file in combo[1:]:\n",
    "                combined_snps = combined_snps.intersection(snp_dict[file])\n",
    "            \n",
    "            # Store the intersection along with the combination and the number of variants\n",
    "            num_intersection = len(combined_snps)\n",
    "            intersections.append((num_intersection, combo, set(file_names) - set(combo)))\n",
    "    \n",
    "    # Sort by the number of intersecting variants (descending order), then by the number of files (more files is better)\n",
    "    intersections.sort(key=lambda x: (-x[0], -len(x[1])))\n",
    "    \n",
    "    # Prepare the results in a DataFrame\n",
    "    results = []\n",
    "    for idx, (num_intersection, best_combination, excluded_files) in enumerate(intersections, 1):\n",
    "        results.append({\n",
    "            \"Rank\": idx,\n",
    "            \"Intersecting SNPs\": num_intersection,\n",
    "            \"Best Combination\": \", \".join(get_filename(file) for file in best_combination),\n",
    "            \"Excluded Files\": \", \".join(get_filename(file) for file in excluded_files)\n",
    "        })\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display the table in the notebook\n",
    "    display(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plug in your bim files from the last notebook down below to see the permutations of overlap amongst your datasets\n",
    "- If one or more of your datasets has one or more arrays, you can run this script on those bim files as well as a handy sanity check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_bim_files = ['path/to/file1.bim', 'path/to/file2.bim', 'path/to/file3.bim']\n",
    "combo(list_of_bim_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are we looking? If we see in our permutations that a singular study or array is causing a sharp decline in your variant counts, that may be cause for concern.\n",
    "---\n",
    "# Up next we have the raw merger of our datasets\n",
    "- Include the studies you think thread the needle in terms of variant counts and your research priorities! \n",
    "\n",
    "# In this section we are going to:\n",
    "- Affix a designated suffix to our sample IDS \n",
    "  - This will help us keep track of who comes from where once everything is merged together\n",
    "- Merged the datasets together without minding for overlap\n",
    "  - We should expect a single set of plink files at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawMerger:\n",
    "    def __init__(self, output_dir, studies, suffixes):\n",
    "        \"\"\"\n",
    "        Initialize the GWAS merger class.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Directory to store output files.\n",
    "            studies (list you make before running): List of study prefixes (e.g., [\"/path/to/study1\", \"/path/to/study2\"]).\n",
    "            suffixes (list you make before running): List of suffixes corresponding to each study (e.g., [\"suffix1\", \"suffix2\"]).\n",
    "        \"\"\"\n",
    "        if len(studies) != len(suffixes):\n",
    "            raise ValueError(\"The number of studies and suffixes must match.\")\n",
    "\n",
    "        self.output_dir = os.path.abspath(output_dir)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.merge_list = os.path.join(self.output_dir, \"merge_list.txt\")\n",
    "        self.variant_sets = {}  # Store variant sets for each study\n",
    "        self.studies = studies  # List of study prefixes\n",
    "        self.suffixes = suffixes  # List of suffixes\n",
    "\n",
    "    def update_ids(self):\n",
    "        \"\"\"\n",
    "        Update IDs in the PLINK files using the provided suffixes.\n",
    "        \"\"\"\n",
    "        for study, suffix in zip(self.studies, self.suffixes):\n",
    "            base_name = os.path.basename(study)\n",
    "            update_ids_file = os.path.join(self.output_dir, f\"{base_name}_update_ids.txt\")\n",
    "            output_prefix = os.path.join(self.output_dir, f\"{base_name}_updated\")\n",
    "\n",
    "            # Create update IDs file\n",
    "            fam_file = f\"{study}.fam\"\n",
    "            with open(fam_file, 'r') as f_in, open(update_ids_file, 'w') as f_out:\n",
    "                for line in f_in:\n",
    "                    parts = line.strip().split()\n",
    "                    f_out.write(f\"{parts[0]} {parts[1]} {parts[0]} {parts[1]}-{suffix}\\n\") #You can attach the suffix with anything you want just change the divdier here {parts[1]}-{suffix}\\n\n",
    "\n",
    "            # Run PLINK to update IDs\n",
    "            subprocess.run([plink_path, \"--bfile\", study,\"--update-ids\", update_ids_file,\"--keep-allele-order\",\"--make-bed\",\"--out\", output_prefix], check=True)\n",
    "\n",
    "            # Add to merge list\n",
    "            with open(self.merge_list, 'a') as f:\n",
    "                f.write(f\"{output_prefix}\\n\")\n",
    "\n",
    "            # Extract variant IDs from the .bim file\n",
    "            bim_file = f\"{study}.bim\"\n",
    "            with open(bim_file, 'r') as f:\n",
    "                variant_ids = {line.strip().split()[1] for line in f}\n",
    "            self.variant_sets[suffix] = variant_ids\n",
    "\n",
    "\n",
    "    def merge_studies(self):\n",
    "        \"\"\"\n",
    "        Merge all PLINK studies into a single dataset.\n",
    "        \"\"\"\n",
    "        with open(self.merge_list, 'r') as f:\n",
    "            files = f.readlines()\n",
    "        if not files:\n",
    "            raise ValueError(\"No files to merge!\")\n",
    "\n",
    "        first_file = files[0].strip()\n",
    "        merge_temp = os.path.join(self.output_dir, \"merge_temp.txt\")\n",
    "        with open(merge_temp, 'w') as f:\n",
    "            f.writelines(files[1:])\n",
    "\n",
    "        base_name = os.path.join(self.output_dir, \"merged_data\")\n",
    "        subprocess.run([plink_path, \"--bfile\", first_file, \"--merge-list\", merge_temp, \"--keep-allele-order\", \"--make-bed\", \"--out\", base_name], check=True)\n",
    "\n",
    "        # Track QC after merging\n",
    "        count_variants(\"Merged\", f\"{base_name}.bim\", f\"{base_name}.fam\", \"After Merge\")\n",
    "        self.current_base = base_name\n",
    "        \n",
    "    def get_output_files(self):\n",
    "        \"\"\"Return the final BIM and FAM files after missingness filtering.\"\"\"\n",
    "        return self.current_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/path/to/output\"\n",
    "studies = [\"/path/to/study1\", \"/path/to/study2\"]\n",
    "suffixes = [\"suffix1\", \"suffix2\"]\n",
    "raw_merger = RawMerger(output_dir, studies, suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_merger.update_ids()\n",
    "raw_merger.merge_studies()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the genotyping rates between when the datasets were on their own versus that final merger, do you see how big a role the intersection of variants plays! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This next section of code is going to now filter out those non-overlapping variants!\n",
    "---\n",
    "# What if say, you have ten datasets and only one dataset does not have a particular variant, seems silly to throw that variant out, right? \n",
    "- Our code will include some flexibility in the percentage of overlap needed for a variant to be kept in the final merged reference panel\n",
    "    - Just remember, the lower your threshold for accepting variants, the higher the missingness will be and the lower the overall genotyping rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IntersectionAnalysis:\n",
    "    def __init__(self, variant_sets, base_name, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the intersection analysis class.\n",
    "        \n",
    "        Args:\n",
    "            variant_sets (dict): Dictionary mapping suffixes to sets of variant IDs.\n",
    "            base_name (str): Prefix of the merged PLINK dataset.\n",
    "            output_dir (str): Directory to store output files.\n",
    "        \"\"\"\n",
    "        self.variant_sets = variant_sets\n",
    "        self.base_name = base_name\n",
    "        self.output_dir = os.path.abspath(output_dir)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.intersection_output = None\n",
    "\n",
    "    def calculate_pairwise_intersections(self):\n",
    "        \"\"\"\n",
    "        Calculate pairwise intersections between datasets.\n",
    "        \"\"\"\n",
    "        pairwise_results = []\n",
    "        labels = list(self.variant_sets.keys())\n",
    "        data_sets = list(self.variant_sets.values())\n",
    "\n",
    "        # Calculate self-intersections\n",
    "        for i, set_a in enumerate(data_sets):\n",
    "            intersection_count = len(set_a & set_a)\n",
    "            pairwise_results.append((labels[i], labels[i], intersection_count, 100.0, 100.0))\n",
    "\n",
    "        # Calculate pairwise intersections\n",
    "        for (i, set_a), (j, set_b) in combinations(enumerate(data_sets), 2):\n",
    "            intersection_count = len(set_a & set_b)\n",
    "            percent_a = (intersection_count / len(set_a)) * 100 if len(set_a) > 0 else 0\n",
    "            percent_b = (intersection_count / len(set_b)) * 100 if len(set_b) > 0 else 0\n",
    "            pairwise_results.append((labels[i], labels[j], intersection_count, percent_a, percent_b))\n",
    "            pairwise_results.append((labels[j], labels[i], intersection_count, percent_b, percent_a))\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        df = pd.DataFrame(pairwise_results, columns=[\"Set1\", \"Set2\", \"IntersectionSize\", \"PercentSet1\", \"PercentSet2\"])\n",
    "\n",
    "        # Display pairwise results\n",
    "        print(\"Pairwise Intersection Results:\")\n",
    "        print(df)\n",
    "        self.pairwise_results = df\n",
    "        return df\n",
    "\n",
    "    def generate_heatmap(self):\n",
    "        \"\"\"\n",
    "        Generate and display a heatmap for pairwise intersection results.\n",
    "        \n",
    "        Args:\n",
    "            pairwise_results (pd.DataFrame): DataFrame containing pairwise intersection results.\n",
    "        \"\"\"\n",
    "       \n",
    "        heatmap_data = self.pairwise_results.pivot(index=\"Set1\", columns=\"Set2\", values=\"PercentSet1\")\n",
    "        heatmap_data = heatmap_data.fillna(0)\n",
    "\n",
    "        \n",
    "        heatmap_data = heatmap_data.where(np.triu(np.ones(heatmap_data.shape)).astype(bool))\n",
    "\n",
    "        # Create the heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"Blues\", cbar_kws={'label': 'Percent in Common'})\n",
    "        plt.title(\"Heatmap with Percent in Common\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "    def get_best_intersection(self, threshold=\"\"):\n",
    "        \"\"\"\n",
    "        Calculate the \"best\" intersection (variants present in most studies).\n",
    "    \n",
    "        Args:\n",
    "            threshold (float): Threshold for the percentage of studies a variant must be present in.\n",
    "    \n",
    "        Returns:\n",
    "            set: Set of variants that meet the threshold.\n",
    "        \"\"\"\n",
    "        # Count how many studies each variant appears in\n",
    "        variant_counts = Counter()\n",
    "        for variant_set in self.variant_sets.values():\n",
    "            # Ensure counts are integers\n",
    "            variant_counts.update({variant: 1 for variant in variant_set})\n",
    "\n",
    "\n",
    "        # Calculate the threshold count\n",
    "        threshold_count = len(self.variant_sets) * threshold\n",
    "\n",
    "        # Select variants that meet the threshold\n",
    "        best_intersection = {variant for variant, count in variant_counts.items() if count >= threshold_count}  # No need to convert count to int\n",
    "\n",
    "        # Calculate the average percentage of how often each variant is shared across studies\n",
    "        total_studies = len(self.variant_sets)\n",
    "        average_percentage = np.mean([(count / total_studies) * 100 for count in variant_counts.values() if count >= threshold_count])  # No need to convert count to int\n",
    "\n",
    "        print(f\"Best intersection includes {len(best_intersection)} variants (present in at least {threshold * 100}% of studies).\")\n",
    "        print(f\"On average, these variants are shared across {average_percentage:.2f}% of studies.\")\n",
    "    \n",
    "        return best_intersection\n",
    "    \n",
    "    def filter_merged_dataset(self, threshold):\n",
    "        \"\"\"\n",
    "        Filter the merged dataset using the optimal intersection.\n",
    "        \"\"\"\n",
    "        best_intersection = self.get_best_intersection(threshold)\n",
    "\n",
    "        # Write the best intersection to a text file\n",
    "        intersection_file = os.path.join(self.output_dir, \"best_intersection.txt\")\n",
    "        with open(intersection_file, 'w') as file:\n",
    "            file.write('\\n'.join(best_intersection))\n",
    "\n",
    "        print(f\"Best intersection saved to: {intersection_file}\")\n",
    "\n",
    "        filtered_prefix = os.path.join(self.output_dir, \"merged_data_filtered\")\n",
    "        filtered_prefix = filtered_prefix.strip()\n",
    "        self.base_name = self.base_name.strip()\n",
    "        intersection_file = intersection_file.strip()\n",
    "\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name, \"--extract\", intersection_file, \"--keep-allele-order\", \"--make-bed\", \"--out\", filtered_prefix],capture_output=True, text=True)\n",
    "        count_variants(\"Merged\", f\"{filtered_prefix}.bim\", f\"{filtered_prefix}.fam\", \"After Intersection\")\n",
    "        self.current_base = filtered_prefix\n",
    "    def get_output_files(self):\n",
    "        return self.current_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_base = raw_merger.get_output_files()\n",
    "\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = IntersectionAnalysis(raw_merger.variant_sets, 'path/to/base/name', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we will take another look at the final overlap of varaints and we can see the percentages of each overlap in a heatmap format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection.calculate_pairwise_intersections()\n",
    "intersection.generate_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with that information in hand, we can plug in the threshold we think is appropriate and plug it in below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection.filter_merged_dataset(DECIMAL OF PERCENTAGE THRESHOLD)\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This next section should look familar to our \"twin\" screening back in the quality control notebook, we have a similar concern as we did of duplicate samples in our final dataset, so let us go ahead and nip them in the bud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwinProcessor:\n",
    "    def __init__(self, study_name, base_name, out_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            study_name : Name of the study.\n",
    "            base_name : Base name of the input files (including full path, e.g., \"path/to/data_base_name\").\n",
    "            out_dir : Directory where all output files will be saved.\n",
    "        \"\"\"\n",
    "        self.study_name = study_name\n",
    "        self.base_name = base_name\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "        self.intermediate_files = []\n",
    "        # Initialize current BIM and FAM files\n",
    "        self.current_base = os.path.basename(base_name)\n",
    "        self.current_bim = f\"{self.current_base}.bim\"\n",
    "        self.current_fam = f\"{self.current_base}.fam\"\n",
    "\n",
    "    def identify_twins(self):\n",
    "        # Create a temporary file for processing\n",
    "        temp_base = os.path.join(self.out_dir, \"temp\")\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name, \"--keep-allele-order\", \"--make-bed\", \"--out\", temp_base], check=True)\n",
    "\n",
    "        # Calculate relatedness using PLINK2\n",
    "        subprocess.run([plink2_path, \"--bfile\", temp_base, \"--make-king-table\", \"--king-table-filter\", \"0.5\", \"--out\", \"relatedness\"], check=True)\n",
    "\n",
    "        # Create twins.txt file with a header\n",
    "        twins_file_path = os.path.join(self.out_dir, \"twins.txt\")\n",
    "        with open(twins_file_path, \"w\") as twins_file:\n",
    "            twins_file.write(\"FID1\\tIID1\\tFID2\\tIID2\\n\")  # Write header\n",
    "\n",
    "            # Extract twin pairs with KING coefficient > 0.5\n",
    "            if os.path.exists(\"relatedness.kin0\"):\n",
    "                with open(\"relatedness.kin0\", \"r\") as kin_file:\n",
    "                    next(kin_file)  # Skip the header line\n",
    "                    for line in kin_file:\n",
    "                        fields = line.strip().split()\n",
    "                        if float(fields[7]) > 0.354:  # KING coefficient in column 8\n",
    "                            twins_file.write(f\"{fields[0]}\\t{fields[1]}\\t{fields[2]}\\t{fields[3]}\\n\")\n",
    "\n",
    "        self.intermediate_files.append(f\"{temp_base}.*\")\n",
    "        self.intermediate_files.append(\"relatedness.kin0\")\n",
    "        self.intermediate_files.append(twins_file_path)\n",
    "        self.twins_file = twins_file_path\n",
    "\n",
    "    def screen_twins(self):\n",
    "        # Ensure twins.txt exists\n",
    "        if not os.path.exists(self.twins_file):\n",
    "            print(\"Twins file does not exist. Creating an empty file.\")\n",
    "            with open(self.twins_file, \"w\") as twins_file:\n",
    "                twins_file.write(\"FID1\\tIID1\\tFID2\\tIID2\\n\")  # Write header\n",
    "\n",
    "        # Run PLINK to calculate missingness\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name, \"--missing\", \"--out\", \"missingness\"], check=True)\n",
    "\n",
    "        # Read missingness data\n",
    "        individual_variants = {}\n",
    "        if os.path.exists(\"missingness.imiss\"):\n",
    "            with open(\"missingness.imiss\", \"r\") as imiss_file:\n",
    "                next(imiss_file)  # Skip the header line\n",
    "                for line in imiss_file:\n",
    "                    fields = line.strip().split()\n",
    "                    fid, iid = fields[0], fields[1]\n",
    "                    non_missing_count = int(fields[4])\n",
    "                    individual_variants[(fid, iid)] = non_missing_count\n",
    "            self.intermediate_files.append(\"missingness.imiss\")\n",
    "\n",
    "        # Initialize lists for twins to remove and tied twins\n",
    "        twins_to_remove = []\n",
    "        twins_tied = []\n",
    "\n",
    "        # Process twins file\n",
    "        with open(self.twins_file, \"r\") as twins_file:\n",
    "            header = next(twins_file)  # Read header\n",
    "            for line in twins_file:\n",
    "                fields = line.strip().split()\n",
    "                twin1 = (fields[0], fields[1])\n",
    "                twin2 = (fields[2], fields[3])\n",
    "                if twin1 in individual_variants and twin2 in individual_variants:\n",
    "                    if individual_variants[twin1] < individual_variants[twin2]:\n",
    "                        twins_to_remove.append(twin2)\n",
    "                    elif individual_variants[twin1] > individual_variants[twin2]:\n",
    "                        twins_to_remove.append(twin1)\n",
    "                    else:\n",
    "                        twins_tied.append((twin1, twin2))\n",
    "\n",
    "        # Create twins_to_remove.txt with header\n",
    "        remove_file_path = os.path.join(self.out_dir, \"twins_to_remove.txt\")\n",
    "        with open(remove_file_path, \"w\") as remove_file:\n",
    "            remove_file.write(\"FID\\tIID\\n\")  # Write header\n",
    "            for fid, iid in twins_to_remove:\n",
    "                remove_file.write(f\"{fid}\\t{iid}\\n\")\n",
    "\n",
    "        # Create twins_tied.txt with header\n",
    "        tied_file_path = os.path.join(self.out_dir, \"twins_tied.txt\")\n",
    "        with open(tied_file_path, \"w\") as tied_file:\n",
    "            tied_file.write(header)  # Write header from twins.txt\n",
    "            for twin1, twin2 in twins_tied:\n",
    "                tied_file.write(f\"{twin1[0]}\\t{twin1[1]}\\t{twin2[0]}\\t{twin2[1]}\\n\")\n",
    "\n",
    "        self.tied = tied_file_path\n",
    "        self.remove = remove_file_path\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def remove_twins(self):\n",
    "        \"\"\"Remove flagged twins from the dataset.\"\"\"\n",
    "        twin_base = os.path.join(self.out_dir, f\"{os.path.basename(self.base_name)}_twins_filtered\")\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name, \"--remove\", self.remove ,\"--keep-allele-order\", \"--make-bed\", \"--out\", twin_base], check=True)\n",
    "\n",
    "        # Update current BIM and FAM files\n",
    "        self.current_base = twin_base\n",
    "        self.current_bim = f\"{twin_base}.bim\"\n",
    "        self.current_fam = f\"{twin_base}.fam\"\n",
    "        count_variants(self.study_name, self.current_bim, self.current_fam, \"Removal of Homozygotic Twins\")\n",
    "        total_manual = subprocess.run([\"wc\", \"-l\", self.tied], capture_output=True, text=True).stdout.strip()\n",
    "        print(f\"Total number of twins neededing manual curation: {total_manual}\")\n",
    "        for file in self.intermediate_files:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "    def get_output_files(self):\n",
    "        return self.current_base    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_base = intersection.get_output_files()\n",
    "\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twins = TwinProcessor(\"Merged Dataset\", \"Path/to/last/basename\", \"Path/to/output/directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twins.identify_twins()\n",
    "twins.screen_twins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this final section, if you want to \"manually curate\" your samples you can manually add them to the twins_to_remove.txt, \n",
    "- Yake a look at your flagged tied twins a bit more closely \n",
    "  - see if the flagged relationships actually make sense in the context of your data and if not it make speak to the quality of the data or simply a bad statstical guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twins.remove_twins()\n",
    "print(tabulate(shared_qc_table, headers=headers, tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So now that we have merged our datasets together, trimmed for only overlapping variants, and removed any duplicate samples, what's next?\n",
    "---\n",
    "# We need to screen for batch effects!\n",
    "- What exactly is a batch effect?\n",
    "# Batch effects are  technical artifacts that introduce non-biological variations in genetic data due to differences in experimental conditions, equipment, or processing across multiple datasets.\n",
    "- Unless you want your groundbreaking genetic discovery to be just a sneaky batch effect in disguise! \n",
    "  - Screening for batch effects ensures that the biological signals you’re studying aren’t overshadowed by technical noise, keeping your data clean and your conclusions reliable!\n",
    "---\n",
    "# What we'll do is screen our datasets for variants statistically with a particular dataset \n",
    "  - Indicating a \"batch effected\" variant\n",
    "# We'll do this through **G**enome-**W**ide **A**ssociation **S**tudy (GWAS)\n",
    "  - We will be running this GWAS through a jack-kniffing statistical method:\n",
    "    - This means for each study we will pull it out as our \"case\" and then pool the significant values.\n",
    "  - We will aslo run our GWAS with the first twenty **P**rincipal **C**omponents (PCs) as covariants:\n",
    "    - We are using PCs to measure the genetic distance between samples.\n",
    "    - This is to ensure that we are not throwing out variants that are associated with a particular ancestry as opposed to a genuine batch effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWAS:\n",
    "    def __init__(self, base_name, output_dir, suffixes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the PCA and GWAS analysis class.\n",
    "        \n",
    "        Args:\n",
    "            base_name (str): Prefix of the merged PLINK dataset.\n",
    "            output_dir (str): Directory to store output files.\n",
    "            studies (list): List of study names (e.g., [\"study1\", \"study2\"]).\n",
    "            suffixes (list): List of suffixes corresponding to each study (e.g., [\"s1\", \"s2\"]).\n",
    "        \"\"\"\n",
    "        self.base_name = base_name\n",
    "        self.output_dir = os.path.abspath(output_dir)\n",
    "        self.suffixes = suffixes\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.base_name = base_name\n",
    "        self.output_dir = os.path.abspath(output_dir)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "\n",
    "    def run_pca(self):\n",
    "        \"\"\"\n",
    "        Run PCA on the merged dataset.\n",
    "        \"\"\"\n",
    "        pca_dir = os.path.join(self.output_dir, \"PCA\")\n",
    "        os.makedirs(pca_dir, exist_ok=True)\n",
    "\n",
    "        # Step 1: Quality Control (QC) for Your Data\n",
    "        print(\"Performing Quality Control on Your Data...\")\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name,\"--maf\", \"0.05\",\"--geno\", \"0.05\",\"--hwe\", \"1e-25\",\"--exclude\", \"range\", genome_gap,\"--keep-allele-order\",\"--make-bed\",\"--out\", os.path.join(pca_dir, \"qc_filtered_your_data\")], check=True)\n",
    "\n",
    "        # Step 2: LD Pruning\n",
    "        print(\"Performing LD Pruning...\")\n",
    "        subprocess.run([plink_path, \"--bfile\", os.path.join(pca_dir, \"qc_filtered_your_data\"),\"--indep-pairwise\", \"50\", \"5\", \"0.2\", \"--out\", os.path.join(pca_dir, \"ld_pruned_ref\")], check=True)\n",
    "        subprocess.run([plink_path, \"--bfile\", os.path.join(pca_dir, \"qc_filtered_your_data\"),\"--extract\", os.path.join(pca_dir, \"ld_pruned_ref.prune.in\"),\"--keep-allele-order\",\"--make-bed\",\"--out\", os.path.join(pca_dir, \"ld_pruned_dataset\")], check=True)\n",
    "\n",
    "        # Step 3: Relatedness Screening\n",
    "        print(\"Performing Relatedness Screening...\")\n",
    "        subprocess.run([plink_path, \"--bfile\", os.path.join(pca_dir, \"ld_pruned_dataset\"),\"--genome\",\"--out\", os.path.join(pca_dir, \"IBD_Projection\")], check=True)\n",
    "        subprocess.run([\"perl\", primus_path,\"-p\", os.path.join(pca_dir, \"IBD_Projection.genome\"),\"--degree_rel_cutoff 3\", \"--no_PR\"], check=True)\n",
    "        with open(os.path.join(pca_dir, \"unrelated_samples.txt\"), 'w') as f:\n",
    "            subprocess.run([\"awk\", \"NR==FNR {iids[$1]; next} $2 in iids\", os.path.join(pca_dir, \"IBD_Projection.genome_PRIMUS/IBD_Projection.genome_unrelated_samples.txt\"), os.path.join(pca_dir, \"ld_pruned_dataset.fam\")], stdout=f, check=True)\n",
    "        with open(os.path.join(pca_dir, \"individuals_related.txt\"), 'w') as f:\n",
    "            subprocess.run([\"grep\", \"-v\", \"-F\", \"-f\", os.path.join(pca_dir, \"unrelated_samples.txt\"),os.path.join(pca_dir, \"ld_pruned_dataset.fam\")], stdout=f, check=True)\n",
    "\n",
    "        # Step 4: Split the Data\n",
    "        print(\"Splitting the Data...\")\n",
    "        subprocess.run([plink_path, \"--bfile\", os.path.join(pca_dir, \"ld_pruned_dataset\"),\"--keep\", os.path.join(pca_dir, \"unrelated_samples.txt\"),\"--keep-allele-order\",\"--make-bed\",\"--out\", os.path.join(pca_dir, \"pruned_unrelated\")], check=True)\n",
    "        subprocess.run([plink_path, \"--bfile\", os.path.join(pca_dir, \"ld_pruned_dataset\"),\"--keep\", os.path.join(pca_dir, \"individuals_related.txt\"),\"--keep-allele-order\",\"--make-bed\",\"--out\", os.path.join(pca_dir, \"pruned_related\")], check=True)\n",
    "\n",
    "        # Step 5: Run PCA on Unrelated Individuals\n",
    "        print(\"Running PCA on Unrelated Individuals...\")\n",
    "        subprocess.run([flash_path, \"--bfile\", os.path.join(pca_dir, \"pruned_unrelated\"),\"--numthreads\", \"12\",\"--outvec\", os.path.join(pca_dir, \"pca_ref.eigenvec\"),\"--outpve\", os.path.join(pca_dir, \"pca_ref.pve\"),\"--outval\", os.path.join(pca_dir, \"pca_ref.eigenval\"),\"--outload\", os.path.join(pca_dir, \"pca_ref.SNPloadings\"),\"--outmeansd\", os.path.join(pca_dir, \"pca_ref.meanSD\"),\"--outpc\", os.path.join(pca_dir, \"pca_ref.PC\"),\"--memory\", \"25000\",\"-d\", \"20\"], check=True)\n",
    "\n",
    "        # Step 6: Project Related Data onto Reference PCA\n",
    "        print(\"Projecting Related Data onto Reference PCA...\")\n",
    "        subprocess.run([\n",
    "            flash_path, \"--bfile\", os.path.join(pca_dir, \"pruned_related\"),\"--numthreads\", \"12\",\"--project\",\"--inmeansd\", os.path.join(pca_dir, \"pca_ref.meanSD\"),\"--inload\", os.path.join(pca_dir, \"pca_ref.SNPloadings\"),\"--outproj\", os.path.join(pca_dir, \"pca_proj.PC\"),\"-v\"], check=True)\n",
    "\n",
    "        # Step 7: Merge PCA Results\n",
    "        print(\"Merging PCA Results...\")\n",
    "        ref_pca = pd.read_csv(os.path.join(pca_dir, \"pca_ref.PC\"), sep='\\s+', header=None)\n",
    "        proj_pca = pd.read_csv(os.path.join(pca_dir, \"pca_proj.PC\"), sep='\\s+', header=None)\n",
    "\n",
    "        # Rename columns\n",
    "        ref_pca.columns = [\"FID\", \"IID\"] + [f\"PC{i}\" for i in range(1, 21)]\n",
    "        proj_pca.columns = [\"FID\", \"IID\"] + [f\"PC{i}\" for i in range(1, 21)]\n",
    "\n",
    "        # Merge PCA results\n",
    "        merged_pca = pd.concat([ref_pca, proj_pca], ignore_index=True)\n",
    "        merged_pca.to_csv(os.path.join(pca_dir, \"pca_merged_results.txt\"), sep=\"\\t\", index=False)\n",
    "\n",
    "        print(\"PCA results merged and saved to:\", os.path.join(pca_dir, \"pca_merged_results.txt\"))\n",
    "\n",
    "    def run_gwas(self):\n",
    "        \"\"\"\n",
    "        Run GWAS analysis for each phenotype derived from the suffixes.\n",
    "        \"\"\"\n",
    "        united_pheno_file = os.path.join(self.output_dir, \"phenotype_united.txt\")\n",
    "    \n",
    "        with open(united_pheno_file, 'w') as f:\n",
    "            for suffix in self.suffixes:  \n",
    "                fam_file = f\"{self.base_name}.fam\"\n",
    "                with open(fam_file, 'r') as f_in:\n",
    "                    for line in f_in:\n",
    "                        parts = line.strip().split()\n",
    "                        phenotype = parts[1].split('-')[-1]  \n",
    "                        f.write(f\"{parts[0]} {parts[1]} {phenotype}\\n\")\n",
    "        unique_phenos = pd.read_csv(united_pheno_file, sep='\\s+', header=None)[2].unique()\n",
    "\n",
    "        for pheno in unique_phenos:\n",
    "            assoc_output = os.path.join(self.output_dir, f\"GWAS_{pheno}\")\n",
    "            subprocess.run([plink_path, \"--bfile\", self.base_name,\"--make-pheno\", united_pheno_file, pheno,\"--logistic\",\"--covar\", os.path.join(self.output_dir, \"PCA/pca_merged_results.txt\"),\"--adjust\",\"--allow-no-sex\",\"--out\", assoc_output], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_base = twins.get_output_files()\n",
    "\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flag = GWAS('Path/to/last/basename', 'output/directory', suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next stage is going to take awhile, so get comfy!\n",
    "Flag.run_pca()\n",
    "Flag.run_gwas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for sticking in there! \n",
    "# Now that we have the raw results of our GWASs,  we will process them.\n",
    "# We will graph these values two different ways:\n",
    "- **Manhattan-Plot** \n",
    "  - A Manhattan plot visually maps genetic associations across the genome, with each point representing a variant and taller peaks highlighting stronger signals of potential biological importance (or in our case, a batch effect).\n",
    "- **QQ Plot**\n",
    "  - A QQ plot compares the distribution of observed p-values to the expected null distribution, helping to identify deviations that may signal batch effects or other technical artifacts in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    def __init__(self, base_name, data_dir, output_dir):\n",
    "        self.base_name = base_name\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.gwas_data = None\n",
    "\n",
    "    def read_gwas_results(self):\n",
    "        # List all .adjusted files in the directory\n",
    "        assoc_files = [f for f in os.listdir(self.data_dir) if f.endswith('.adjusted')]\n",
    "        gwas_data_list = []\n",
    "\n",
    "        for adjusted_file in assoc_files:\n",
    "            # Construct the corresponding logistic file name\n",
    "            logistic_file = adjusted_file.replace('.assoc.logistic.adjusted', '.assoc.logistic')\n",
    "            logistic_path = os.path.join(self.data_dir, logistic_file)\n",
    "\n",
    "            # Check if the logistic file exists\n",
    "            if not os.path.exists(logistic_path):\n",
    "                raise FileNotFoundError(f\"Matching logistic file not found for: {adjusted_file}\")\n",
    "\n",
    "            # Read the adjusted file (UNADJ values)\n",
    "            df_adjusted = pd.read_csv(os.path.join(self.data_dir, adjusted_file), sep='\\s+')  # Use whitespace as separator\n",
    "            if not all(col in df_adjusted.columns for col in ['CHR', 'SNP', 'UNADJ']):\n",
    "                raise ValueError(f\"File {adjusted_file} does not have required columns: CHR, SNP, UNADJ\")\n",
    "            df_adjusted = df_adjusted[['CHR', 'SNP', 'UNADJ']]\n",
    "            df_adjusted.rename(columns={'UNADJ': 'P'}, inplace=True)  # Rename UNADJ to P\n",
    "\n",
    "            # Read the logistic file (BP values)\n",
    "            df_logistic = pd.read_csv(logistic_path, sep='\\s+')  # Use whitespace as separator\n",
    "            if len(df_logistic.columns) < 3:\n",
    "                raise ValueError(f\"File {logistic_file} does not have at least 3 columns (expecting BP in column 3)\")\n",
    "            df_logistic = df_logistic.iloc[:, :3]  # Select the first 3 columns\n",
    "            df_logistic.columns = ['CHR', 'SNP', 'BP']  # Ensure proper column names\n",
    "\n",
    "            # Merge BP into the adjusted file\n",
    "            df = pd.merge(df_adjusted, df_logistic, on=['CHR', 'SNP'], how='left')\n",
    "\n",
    "            # Check for missing BP values\n",
    "            if df['BP'].isna().any():\n",
    "                print(f\"Warning: Some SNPs in {adjusted_file} are missing BP information from {logistic_file}\")\n",
    "\n",
    "            gwas_data_list.append(df)\n",
    "\n",
    "        # Combine all GWAS results into a single dataframe\n",
    "        self.gwas_data = pd.concat(gwas_data_list, ignore_index=True)\n",
    "\n",
    "    def generate_manhattan_plot(self):\n",
    "        if self.gwas_data is None:\n",
    "            raise ValueError(\"GWAS data not loaded. Please call read_gwas_results() first.\")\n",
    "\n",
    "        # Create Manhattan plot using qqman\n",
    "        figure, axes = plt.subplots(figsize=(16, 6))\n",
    "        qqman.manhattan(\n",
    "            self.gwas_data,\n",
    "            ax=axes,\n",
    "            suggestiveline = -np.log10(1e-5),  # Threshold that should give someone pause about a variant while still technically fine\n",
    "            genomewideline = -np.log10(5e-8),  # Genome-wide significance threshold\n",
    "            cmap=plt.get_cmap(\"jet\"),\n",
    "            cmap_var=10,\n",
    "            xrotation=45,   # Rotate x-axis labels by 45 degrees\n",
    "        )\n",
    "        plt.show()\n",
    "    def generate_QQ_plot(self):\n",
    "        if self.gwas_data is None:\n",
    "            raise ValueError(\"GWAS data not loaded. Please call read_gwas_results() first.\")\n",
    "        figure, axes = plt.subplots(figsize=(8, 8))  # Adjust the figure size as needed\n",
    "        # Generate the QQ plot\n",
    "        qqman.qqplot(\n",
    "            self.gwas_data['P'],  \n",
    "                ax=axes,              \n",
    "                marker='o',           \n",
    "                title='QQ Plot',      \n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def process_significant_snps(self):\n",
    "        concatenated_snps_file = os.path.join(self.output_dir, 'all_significant_snps.txt')\n",
    "        \n",
    "        # List to store counts for each file\n",
    "        counts = []\n",
    "\n",
    "        # DataFrame to store all significant SNPs\n",
    "        all_significant_snps = pd.DataFrame()\n",
    "\n",
    "        for adjusted_file in os.listdir(self.data_dir):\n",
    "            if adjusted_file.endswith('.adjusted'):\n",
    "                # Read the adjusted file\n",
    "                df = pd.read_csv(os.path.join(self.data_dir, adjusted_file), sep='\\s+')\n",
    "                \n",
    "                # Filter significant SNPs\n",
    "                significant_snps = df[df['BONF'] < 5e-8]['SNP'].drop_duplicates()\n",
    "                \n",
    "                # Append to the all_significant_snps DataFrame\n",
    "                all_significant_snps = pd.concat([all_significant_snps, significant_snps], ignore_index=True)\n",
    "                \n",
    "                # Count the number of significant SNPs\n",
    "                count = len(significant_snps)\n",
    "                counts.append((adjusted_file, count))\n",
    "\n",
    "        # Drop duplicates from the concatenated DataFrame\n",
    "        all_significant_snps.drop_duplicates(inplace=True)\n",
    "        \n",
    "        # Save the concatenated SNPs to a single file\n",
    "        all_significant_snps.to_csv(concatenated_snps_file, index=False, header=False)\n",
    "        \n",
    "        # Save the concatenated SNPs as a class attribute\n",
    "        self.significant_snps = concatenated_snps_file\n",
    "\n",
    "        # Print the counts as a table in the notebook\n",
    "        counts_df = pd.DataFrame(counts, columns=['File', 'Significant SNPs'])\n",
    "        display(counts_df)\n",
    "\n",
    "        print(f\"Processing complete. SNPs that need to be removed due to batch effects can be found: {concatenated_snps_file}\")\n",
    "    \n",
    "    def remove_batchs(self, base):\n",
    "        Final_base= os.path.join(self.output_dir, base)\n",
    "        subprocess.run([plink_path, \"--bfile\", self.base_name, \"--exclude\", self.significant_snps, \"--keep-allele-order\", \"--make-bed\", \"--out\", Final_base], check=True)\n",
    "        self.base_name = Final_base\n",
    "\n",
    "    def cleanup_intermediate_files(self):\n",
    "        # Get the base name without the directory path\n",
    "        final_base_name = os.path.basename(self.base_name)\n",
    "\n",
    "        # List all files to delete based on patterns\n",
    "        patterns_to_delete = [\n",
    "            os.path.join(self.output_dir, '*.bed'),\n",
    "            os.path.join(self.output_dir, '*.bim'),\n",
    "            os.path.join(self.output_dir, '*.fam'),\n",
    "            os.path.join(self.output_dir, '*log'),\n",
    "            os.path.join(self.output_dir, '*update*'),\n",
    "            os.path.join(self.output_dir, 'GWAS*'),\n",
    "            os.path.join(self.output_dir, 'id_mapping.txt'),\n",
    "            os.path.join(self.output_dir, 'merged*'),\n",
    "            os.path.join(self.output_dir, '*united*'),\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns_to_delete:\n",
    "            for file_path in glob.glob(pattern):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                # Skip files that match the final base_name (e.g., final_base.bed, final_base.bim, final_base.fam)\n",
    "                if not file_name.startswith(final_base_name):\n",
    "                    os.remove(file_path)\n",
    "        print(\"You have made it through the merger! Happy Hunting!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_base = twins.get_output_files()\n",
    "print(f\"Final base file: {final_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = BatchProcessor('path/to/last/basename','path/to/GWAS/output', 'path/to/output/directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us start with the manhattan plot since this will give us the best image of where we are at in terms of variants significantly associated with a particular study.\n",
    "---\n",
    "- For the manhattan plot there will be two lines:\n",
    "  - One showing the line for genome-wide significane\n",
    "  - A lower \"suspicion\" line, alerting us to variants approaching significance\n",
    "- For both the manhattan plot and QQ plot we will be looking at the unadjusted P-values (those not considering ancestry). We will handle that in the last step of this class but keep that in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.read_gwas_results()\n",
    "Batch.generate_manhattan_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let us move on to the QQ plot, if there was no significane in our variants we should expect to see a fairly straight line. If we see a leftward skew it can be telling us that there are some significant variants to be found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.generate_QQ_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up to this point we have been looking at the unadjusted P-values, but we didn't do all that processing for nothing! \n",
    "# Let us now grab the variants that are *truly* batch effected (i.e., not an ancestry-based variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.process_significant_snps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As you can see, the value of truly batch effected variants is smaller than what the manhattan plot would have suggested! \n",
    "- We have made it to the end! Lastly we will screen out these significant variants and clean up the intermediate files and then you are off to do amazing thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.remove_batchs(\"/home/belleza/scratch/Tutorial/test/FINAL_FILE_NAME\")\n",
    "Batch.cleanup_intermediate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch.remove_batchs(\"path_to_output_directory/FINAL_FILE_NAME\")\n",
    "Batch.cleanup_intermediate_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You did it! You have successfully lifted over, quality controlled, and merged multiple open-access datasets to make your very own reference panel!\n",
    "# Use it in good health!\n",
    "---\n",
    "- If you just can't get enough there are some optional tasks that can be done below!\n",
    "  - They include:\n",
    "    - Removing the suffixes we added if you do not want them in the final dataset\n",
    "    - Changing your underscores to a \".\" (If you convert your data to a VCF it will conjoin your FID and IID as FID_IID, so if there underscores in the FID or IID, this can cause some headaches)\n",
    "    - Convert your data to **V**ariant **C**all **F**ormat (VCF), a single file format commonly used bioinformatics research!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional Final Processing steps\n",
    "class Optional:\n",
    "    def __init__(self, base_name, output_dir):\n",
    "        self.base_name = base_name\n",
    "        self.output_dir = output_dir \n",
    "        self.current_base = base_name    \n",
    "        \n",
    "    #Remove the suffix we added (Will not work if you have not deleted duplicate samples [tied_twins])\n",
    "    def Suffix_remover(self, base):\n",
    "        # Construct the path to the .fam file\n",
    "        fam_file = f\"{self.base_name}.fam\"\n",
    "\n",
    "        # Define the final output base name\n",
    "        Final_base = os.path.join(self.output_dir, base)\n",
    "\n",
    "        # Define the ID mapping file path\n",
    "        id_mapping_file = os.path.join(self.output_dir, \"id_mapping.txt\")\n",
    "\n",
    "        # Write the ID mapping to the file\n",
    "        with open(fam_file, 'r') as infile, open(id_mapping_file, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                # Split the line into FID and IID (first two columns)\n",
    "                fid, iid = line.strip().split()[:2]\n",
    "\n",
    "                # Remove anything after the final '.' in the IID\n",
    "                new_iid = iid.rsplit('.', 1)[0]  # Split on the last '.' and take the first part\n",
    "\n",
    "                # Write the old FID, old IID, new FID (unchanged), and new IID to the file\n",
    "                outfile.write(f\"{fid} {iid} {fid} {new_iid}\\n\")\n",
    "\n",
    "        # Run PLINK to update IDs using the generated mapping file\n",
    "        subprocess.run([plink_path,'--bfile', self.current_base,'--update-ids', id_mapping_file,'--make-bed','--keep-allele-order','--out', Final_base],check=True)\n",
    "\n",
    "        # Update the current base to the new base\n",
    "        self.current_base = Final_base\n",
    "\n",
    "        # Delete the ID mapping file after PLINK has finished\n",
    "        os.remove(id_mapping_file)\n",
    "\n",
    "\n",
    "            \n",
    "    #Change all _ to \".\"\n",
    "    def Underscore_Remover(self, base):\n",
    "        fam_file = f\"{self.base_name}.fam\"\n",
    "        id_mapping_file = os.path.join(self.output_dir, \"id_mapping.txt\")\n",
    "        Final_base= os.path.join(self.output_dir, base)\n",
    "        with open(fam_file, 'r') as infile, open(id_mapping_file, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                # Split the line into FID and IID (first two columns)\n",
    "                fid, iid = line.strip().split()[:2]\n",
    "\n",
    "                # Reconstruct FID by replacing underscores with periods\n",
    "                new_fid = fid.replace('_', '.')\n",
    "\n",
    "                # Reconstruct IID by replacing underscores with periods\n",
    "                new_iid = iid.replace('_', '.')\n",
    "\n",
    "                # Write the old FID, old IID, new FID, and new IID to the output file\n",
    "                outfile.write(f\"{fid} {iid} {new_fid} {new_iid}\\n\")\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--update-ids', id_mapping_file, '--make-bed', '--keep-allele-order', '--out', Final_base], check=True)\n",
    "        self.current_base = Final_base\n",
    "    #Convert to VCF\n",
    "\n",
    "    def VCFMaker(self, base):\n",
    "        Final_base= os.path.join(self.output_dir, base)\n",
    "        subprocess.run([plink_path, '--bfile', self.current_base, '--recode', 'vcf', '--out', Final_base], check=True)\n",
    "        self.current_base = Final_base\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WrapUp = Optional(\"Path/to/final/basename\", \"Path/to/output/directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suffix Remover\n",
    "WrapUp.Suffix_remover(\"path/to/ouput/basename\") #If you want the name to be the same, input Path/to/final/basename that you plugged in when setting up WrapUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Underscore Removal\n",
    "WrapUp.Underscore_Remover(\"path/to/ouput/basename\") #If you want the name to be the same, input Path/to/final/basename that you plugged in when setting up WrapUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to VCF\n",
    "WrapUp.VCFMaker(\"path/to/ouput/basename\") #If you want the name to be the same, input Path/to/final/basename that you plugged in when setting up WrapUP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
